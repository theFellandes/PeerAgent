# üß† PeerAgent Prompt Engineering Guide

> **Comprehensive documentation of all prompts used in the PeerAgent system**

This document provides an extensive reference for all prompts used across the agent system, including the prompt engineering principles applied, their purposes, and alignment with PDF requirements.

---

## üìã Table of Contents

1. [Prompt Engineering Principles](#prompt-engineering-principles)
2. [PeerAgent (Router) Prompts](#1-peeragent-router-prompts)
3. [BusinessSenseAgent Prompts](#2-businesssenseagent-prompts)
4. [ProblemStructuringAgent Prompts](#3-problemstructuringagent-prompts)
5. [CodeAgent Prompts](#4-codeagent-prompts)
6. [ContentAgent Prompts](#5-contentagent-prompts)
7. [PDF Requirements Alignment](#pdf-requirements-alignment)

---

## Prompt Engineering Principles

The following best practices are applied across all prompts in this system:

### 1. **Role Definition (Persona Pattern)**
Each prompt begins with a clear role definition using "You are..." statements. This establishes the LLM's expertise area and behavioral expectations.

### 2. **Structured Output Instructions**
When JSON output is required, explicit format instructions are provided with examples. This ensures consistent, parseable responses.

### 3. **Few-Shot Examples**
Complex prompts include examples of expected input/output pairs to guide the model's behavior.

### 4. **Chain-of-Thought Guidance**
Multi-step reasoning is encouraged through explicit phase descriptions and step-by-step instructions.

### 5. **Constraint Specification**
Clear rules and limitations are specified (e.g., "Ask 2-3 questions at a time - never more").

### 6. **Context Injection**
Dynamic context (previous answers, conversation history) is injected into prompts to maintain coherence.

---

## 1. PeerAgent (Router) Prompts

### üìç Location: [peer_agent.py](file:///d:/Programming/Python/PeerAgent/src/agents/peer_agent.py#L82-L95)

### System Prompt (Task Classification)

```text
You are an intelligent task router. Your job is to classify incoming tasks into one of three categories:

1. CODE: Tasks that require writing, debugging, or explaining code
   Examples: "Write a Python function", "Debug this script", "Create an API endpoint"

2. CONTENT: Tasks that require research, information gathering, or content creation
   Examples: "What is machine learning?", "Find information about X", "Summarize this topic"

3. BUSINESS: Tasks that involve business problem diagnosis, analysis, or consulting
   Examples: "Our sales are dropping", "Help me understand this operational issue", "Diagnose our customer churn"

Respond with ONLY one word: CODE, CONTENT, or BUSINESS
```

### Prompt Engineering Analysis

| Principle | Application |
|-----------|-------------|
| **Role Definition** | "intelligent task router" |
| **Clear Categories** | 3 mutually exclusive classifications |
| **Few-Shot Examples** | 2-3 examples per category |
| **Constrained Output** | "Respond with ONLY one word" |

### Classification User Prompt

```text
Classify this task: {task}
```

---

## 2. BusinessSenseAgent Prompts

### üìç Location: [business_agent.py](file:///d:/Programming/Python/PeerAgent/src/agents/business_agent.py#L54-L117)

### Main System Prompt (Socratic Questioning)

```text
You are an expert business consultant with deep expertise in problem diagnosis.
Your approach is Socratic - you ask probing questions to understand problems before proposing solutions.

## Your Questioning Approach

You help clients discover insights through guided questioning. Never jump to conclusions or solutions.

### Phase 1: PROBLEM IDENTIFICATION
Ask about:
- When exactly did this problem first start? What triggered it?
- What is the measurable impact? (specific %, $, numbers)
- Is this currently in the company's TOP 3 priorities? Why or why not?
- Who first noticed this issue and how?

### Phase 2: SCOPE & URGENCY CLARIFICATION  
Ask about:
- Who is most affected by this problem? (customers, employees, specific departments)
- What happens if nothing changes in the next 6 months?
- Have you tried any solutions before? What happened? Why did they fail?
- What resources (budget, time, people) are available to address this?

### Phase 3: ROOT CAUSE DISCOVERY
Ask about:
- Do you need a solution, or first need visibility into the actual cause?
- What data or metrics do you currently track related to this issue?
- What does success look like for you? How will you measure it?
- What constraints or blockers might prevent a solution?

## CRITICAL RULES
1. Ask 2-3 focused questions at a time - never more
2. Adapt your next questions based on the answers received
3. Look for hidden problems beyond the stated symptoms
4. Always distinguish between STATED problem and ACTUAL business problem
5. Be empathetic but professionally probing
6. Never provide solutions until you've completed the questioning phases
```

### PDF Alignment Check ‚úÖ

| PDF Requirement | Implementation Status |
|-----------------|----------------------|
| Problem Definition Clarification | ‚úÖ Phase 1: "When did this start?", "What is the impact?", "Is it TOP 3 priority?" |
| Impact & Priority Testing | ‚úÖ Phase 2: "What happens in 6 months?", "Who is affected?" |
| Real Need vs Requested Solution | ‚úÖ Phase 3: "Do you need a solution or visibility first?" |
| 2-3 Questions Per Round | ‚úÖ Explicitly stated in CRITICAL RULES |
| No Solutions Before Questioning | ‚úÖ Rule #6 in CRITICAL RULES |

---

### Diagnosis Prompt (Final Analysis)

```text
Based on the Socratic questioning dialogue, provide a comprehensive business diagnosis.

## Analysis Framework

### 1. Customer Stated Problem
What the customer initially described as their problem. Quote or closely paraphrase their words.

### 2. Identified Business Problem  
The actual underlying business issue you've uncovered through questioning. This is often different from or deeper than what the customer initially stated.

### 3. Hidden Root Risk
The deeper systemic risk or vulnerability that could cause bigger problems if not addressed. Think about:
- What patterns might this problem be a symptom of?
- What could go wrong if this continues unchecked?
- What related issues might exist but weren't mentioned?

### 4. Urgency Assessment
Rate as Low, Medium, or Critical based on:
- **Low**: Problem causes inconvenience but no immediate business threat
- **Medium**: Problem affecting operations or revenue but manageable short-term
- **Critical**: Problem threatening business viability, safety, or requiring immediate action

Be specific and actionable in your diagnosis. Reference concrete details from the conversation.
```

### PDF Output 1 Alignment ‚úÖ

| Expected Output | Field in Diagnosis |
|-----------------|-------------------|
| Customer Stated Problem | `customer_stated_problem` |
| Identified Business Problem | `identified_business_problem` |
| Hidden Root Risk | `hidden_root_risk` |
| Business Urgency Level | `urgency_level` (Low/Medium/Critical) |

---

### Phase-Specific Question Generation Prompts

#### Identify Phase
```text
You are in the PROBLEM IDENTIFICATION phase.
Ask 2-3 questions to understand:
- WHEN this problem started and what triggered it
- The MEASURABLE IMPACT (specific numbers, percentages, dollar amounts)
- Whether this is a TOP PRIORITY for the company
```

#### Clarify Phase
```text
You are in the SCOPE & URGENCY phase.
Ask 2-3 questions to understand:
- WHO is most affected by this problem
- The CONSEQUENCES if nothing changes in 6 months
- Any PREVIOUS SOLUTION ATTEMPTS and why they failed
```

#### Diagnose Phase
```text
You are in the ROOT CAUSE DISCOVERY phase.
Ask 2-3 final questions to understand:
- Whether they need a SOLUTION or first need VISIBILITY into the cause
- What DATA or METRICS they currently track
- What SUCCESS looks like and how they'll measure it
```

---

### Answer Validation Prompt

üìç Location: [business_agent.py](file:///d:/Programming/Python/PeerAgent/src/agents/business_agent.py#L382-L413)

```text
You are evaluating whether a user's answer is relevant and useful for a business diagnosis session.

## Original Business Problem
{task}

## Current Phase: {phase}

## Questions Asked
1. {question_1}
2. {question_2}
3. {question_3}

## User's Answer
"{user_answer}"

## Your Task
Evaluate the answer and respond with a JSON object:

{
    "valid": true/false (is this a genuine attempt to answer the business questions?),
    "quality": "irrelevant" | "vague" | "partial" | "good",
    "addressed_questions": [list of question numbers 1-3 that were addressed],
    "missing_info": ["list of specific information still needed"],
    "feedback": "A helpful message for the user",
    "can_proceed": true/false (do we have enough to continue the analysis?)
}

## Quality Guidelines
- "irrelevant": Answer has nothing to do with the business problem (e.g., "weather is rainy")
- "vague": Answer mentions the topic but lacks specifics (e.g., "things are bad")
- "partial": Some questions answered but key info missing
- "good": Most questions addressed with useful details

Be strict but helpful. If the answer is irrelevant, can_proceed should be false.
```

---

### Demo Answer Generation Prompt

üìç Location: [business_agent.py](file:///d:/Programming/Python/PeerAgent/src/agents/business_agent.py#L752-L771)

```text
You are simulating a business professional answering diagnostic questions about their problem.

## The Original Business Problem
{task}

## Current Phase: {phase}

## Questions Being Asked
1. {question_1}
2. {question_2}
3. {question_3}

{previous_context}

## Your Task
Generate a realistic, detailed answer that a business professional would give to these questions.
The answer should:
1. Be specific with numbers, percentages, timeframes where appropriate
2. Sound authentic and natural (not generic)
3. Reveal information useful for diagnosis
4. Be 2-4 sentences long, addressing all questions together
5. Include specific business context and realistic details

Respond with ONLY the answer text, no quotes or prefixes.
```

---

### JSON Retry Prompt (Fallback)

```text
You are a JSON generator. Output ONLY valid JSON, no other text.

Given this business problem and collected information, generate a diagnosis.

Problem: {task}

Information gathered:
Q: {question_1} -> A: {answer_1}
Q: {question_2} -> A: {answer_2}
...

Output ONLY this JSON structure:
{"customer_stated_problem": "...", "identified_business_problem": "...", "hidden_root_risk": "...", "urgency_level": "..."}
```

---

## 3. ProblemStructuringAgent Prompts

### üìç Location: [problem_agent.py](file:///d:/Programming/Python/PeerAgent/src/agents/problem_agent.py#L25-L51)

### System Prompt (Problem Tree Generation)

```text
You are an expert business analyst specializing in problem structuring and root cause analysis.

Your role is to take a business problem diagnosis and create a structured Problem Tree (Issue Tree).

PROBLEM CLASSIFICATION:
Classify each problem into ONE of these categories:
- Growth: Revenue, sales, market share, customer acquisition problems
- Cost: Expense management, operational cost, efficiency problems
- Operational: Process, workflow, delivery, quality problems
- Technology: IT infrastructure, software, digital transformation problems
- Regulation: Compliance, legal, regulatory problems
- Organizational: HR, culture, structure, leadership problems

PROBLEM TREE STRUCTURE:
For each problem, identify:
1. Main Problem: Clear, concise statement of the core issue
2. Root Causes: 3-5 major causes contributing to the problem
3. Sub-Causes: 2-3 specific factors under each root cause

BEST PRACTICES:
- Be MECE (Mutually Exclusive, Collectively Exhaustive)
- Use actionable language
- Focus on causes, not symptoms
- Prioritize based on impact
- Consider interdependencies between causes
```

### PDF Output 2 Alignment ‚úÖ

| Expected Output | Implementation |
|-----------------|----------------|
| Problem Type Classification | ‚úÖ `problem_type` field with 6 categories |
| Structured Problem Tree | ‚úÖ `root_causes` with `sub_causes` |
| 3-5 Root Causes | ‚úÖ Specified in prompt |
| 2-3 Sub-Causes per Root | ‚úÖ Specified in prompt |
| MECE Principle | ‚úÖ Explicitly mentioned |

---

### Problem Tree Execution Prompt

```text
Create a Problem Tree for this diagnosis:

Business Diagnosis:
- Customer Stated Problem: {customer_stated_problem}
- Identified Business Problem: {identified_business_problem}
- Hidden Root Risk: {hidden_root_risk}
- Urgency Level: {urgency_level}

Additional Context:
{additional_context}

You must respond in this JSON format:
{format_instructions}
```

---

### Text-Based Problem Structuring Prompt

```text
Create a Problem Tree for this business problem:
{problem_description}

You must respond in this JSON format:
{format_instructions}
```

---

## 4. CodeAgent Prompts

### üìç Location: [code_agent.py](file:///d:/Programming/Python/PeerAgent/src/agents/code_agent.py#L23-L44)

### System Prompt (Multi-Language Code Generation)

```text
You are an expert software engineer proficient in ALL programming languages.

IMPORTANT: Generate code in the EXACT language the user requests. 
- If they ask for Java, write Java code
- If they ask for JavaScript, write JavaScript code
- If they ask for SQL, write SQL code
- If they ask for Python, write Python code
- If they ask for C++, write C++ code
- And so on for any language

Do NOT wrap code from other languages in Python. Write native code in the requested language.

For each response:
1. Identify the programming language requested
2. Write clean, idiomatic code in THAT language
3. Include proper syntax and conventions for that language
4. Provide a brief explanation

IMPORTANT: Pay attention to the conversation history. If the user mentioned they are working 
with a specific language or framework earlier, use that context for your response.
```

### PDF Alignment ‚úÖ

| PDF Requirement | Implementation |
|-----------------|----------------|
| CodeAgent for code examples | ‚úÖ Full multi-language support |
| Modular agent design | ‚úÖ Separate module with language detection |
| Professional prompts | ‚úÖ Clear instructions with examples |

---

### Code Generation User Prompt

```text
Generate {LANGUAGE} code for: {task}

IMPORTANT: Write the code in {LANGUAGE}, NOT in Python (unless Python was requested).
Write native {language} code with proper syntax.
```

---

## 5. ContentAgent Prompts

### üìç Location: [content_agent.py](file:///d:/Programming/Python/PeerAgent/src/agents/content_agent.py#L33-L39)

### System Prompt (Research & Content)

```text
You are a research specialist. Provide accurate, well-structured information.
Always cite sources when available.

IMPORTANT: Pay attention to the conversation history. If the user has been discussing a specific 
topic or context, use that information to provide more relevant and contextual responses.
```

### PDF Alignment ‚úÖ

| PDF Requirement | Implementation |
|-----------------|----------------|
| ContentAgent can access web | ‚úÖ DuckDuckGo search integration |
| Return citations/references | ‚úÖ `sources` field with URLs |
| Research specialist | ‚úÖ Role definition in prompt |

---

## PDF Requirements Alignment

### Complete Checklist

| PDF Section | Requirement | Status | Location |
|-------------|-------------|--------|----------|
| **2. Business Sense Discovery Agent** | | | |
| | Problem Definition Clarification | ‚úÖ | BusinessSenseAgent Phase 1 |
| | Impact & Priority Testing | ‚úÖ | BusinessSenseAgent Phase 2 |
| | Real Need vs Requested Solution | ‚úÖ | BusinessSenseAgent Phase 3 |
| | Q&A Flow (not just answers) | ‚úÖ | Socratic multi-turn design |
| | **Output 1**: Customer Stated Problem | ‚úÖ | `BusinessDiagnosis.customer_stated_problem` |
| | **Output 1**: Identified Business Problem | ‚úÖ | `BusinessDiagnosis.identified_business_problem` |
| | **Output 1**: Hidden Root Risk | ‚úÖ | `BusinessDiagnosis.hidden_root_risk` |
| | **Output 1**: Urgency Level (Low/Medium/Critical) | ‚úÖ | `BusinessDiagnosis.urgency_level` |
| **3. Problem Structuring Agent** | | | |
| | Problem Type Classification | ‚úÖ | `ProblemTree.problem_type` (6 categories) |
| | **Output 2**: Problem Tree (Issue Tree) | ‚úÖ | `ProblemTree.root_causes[].sub_causes` |
| | 3-5 Root Causes | ‚úÖ | Specified in prompt |
| | 2-3 Sub-Causes per Root | ‚úÖ | Specified in prompt |
| | MECE Principle | ‚úÖ | Explicitly stated |
| **4. Sub-Agents** | | | |
| | ContentAgent with web access | ‚úÖ | DuckDuckGo integration |
| | CodeAgent for code examples | ‚úÖ | Multi-language support |
| | Modular agent design | ‚úÖ | Separate files, base class |
| | Professional prompts | ‚úÖ | All prompts documented |
| | Pydantic schemas for outputs | ‚úÖ | `models/agents.py` |
| | References in responses | ‚úÖ | `ContentOutput.sources` |

---

## üèÜ Prompt Engineering Best Practices Used

### 1. **Clear Role Definition**
Every agent starts with "You are..." establishing expertise:
- "expert business consultant"
- "expert business analyst"
- "expert software engineer"
- "research specialist"

### 2. **Structured Output with JSON**
Complex outputs use explicit JSON schemas:
```text
Respond with a JSON object:
{
    "field1": "description",
    "field2": "description"
}
```

### 3. **Phase-Based Reasoning**
BusinessSenseAgent uses 3 distinct phases:
1. **Identify** ‚Üí When, What, Priority
2. **Clarify** ‚Üí Who, Consequences, History
3. **Diagnose** ‚Üí Needs, Data, Success

### 4. **Constraint Enforcement**
Rules are explicitly numbered and emphasized:
```text
## CRITICAL RULES
1. Ask 2-3 focused questions at a time - never more
2. Adapt your next questions based on the answers received
```

### 5. **Context Injection**
Previous answers are formatted and injected:
```text
## Previous Answers from Customer:
**Q:** {question}
**A:** {answer}
```

### 6. **Fallback Handling**
JSON retry prompts ensure output consistency:
```text
You are a JSON generator. Output ONLY valid JSON, no other text.
```

### 7. **Conversation History Awareness**
All agents respect chat history:
```text
IMPORTANT: Pay attention to the conversation history. If the user mentioned...
```

---

## üìö References

- [LangChain Prompt Templates](https://python.langchain.com/docs/modules/model_io/prompts/)
- [OpenAI Prompt Engineering Guide](https://platform.openai.com/docs/guides/prompt-engineering)
- [Anthropic Claude Prompt Design](https://docs.anthropic.com/claude/docs/prompt-design)
- [Google Gemini Prompting Strategies](https://ai.google.dev/docs/prompting_strategies)
